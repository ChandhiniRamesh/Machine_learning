mean
variance
skewness
Kurtosis

mean,median,mode
variance,SD


mean -> average
median -> midpoint

10,20,30,40,50 -> mean=150/5=30
               -> median=30

15,000 -2,00,000

		number of occurence of an event
probability = -------------------------------------
		total number of occurence 

why ML?

sample = { (1,1),(1,2)........ (6,6)}

A -> B => dep

A B => indep

A  c - B => conditionally dep

			    P(A and B)
conditional probability =  -------------- = P(A/B)  = 0.35/0.7 = 0.5
			      P(B)

			P(A) P(B/A)
Navie Baye's, P(A/B) = --------------
			   P(B)

70% like chocolate
35% like strawberry and chocolate 

those who like chocolate also like strawberry -> P(strawberry/chocolate)

SMHT - yes?/no?

		P(yes) P(SMHT/yes)
P(yes/SMHT) = -----------------------
		P(S).P(M).P(H).P(T)

		P(no) P(SMHT/no)
P(no/SMHT) = ------------------------
		P(S).P(M).P(H).P(T)

Entropy -> measure of randomness (thermodynamics)

Message/Information --> sun rises in the east , entropy is the measure of randomness 

Entropy -> measure of information content

Entropy (H) = - P log_2 (P)

	     log_10 (x)		log_10 (x)
log_2 (x) = --------------  = --------------
	     log_10 (2)		  0.3010

p1 = 0.5

h = - 0.5 log2 (0.5)

     log10 (0.5)       -0.3010
h = ------------  =  ------------ = -1
      0.3010	        0.3010

H1 = -0.5 * -1 = 0.5

algorithm using entropy is decision tree

Information = High  --> Uncertain or most unlikely event

Information = Low   --> Certain or most likely event

Information = twice --> independent event or mutually exclusive 

Stochostic Process 

part of statistics which deals with random process/random events

			random variable (x)
		     __________|____________
		 Continuous		  Discrete

Probability Distribution function (X)

poisson,exponential,gaussian,binomial,uniform 

60% of people who purchase sports cars are men.  If 10 sports car owners are randomly selected, find the probability that exactly 7 are men.

probability mass function (PMF) -> discrete -> summation
probability density function(PDF) -> continuous -> integeral
marginal probability => 0-60 -> 5seconds,0-20 seconds?
joint probability (X and Y) 

HYPOTHESIS

---------------------------------------------------------------------------------------------------------------------------

						Linear Algebra

vectors and matrix
			-5 _______________________ 0

scalar  -> magnitude = 5

vector  -> magnitude and direction => (-5,0)

matrix  -> 2D data 

Tensor  -> 3D data


Matrix => by solving Ax = b,we can get to know the given matrix has a solution?

A -> input matrix
b -> constant value
x -> variable

how to find a solution?
1. Rank method
2. Carmer's rule

Determinant:- square matrix (m x n, m=n) i.e., m=> row, n=> column

     2  4  7       x  x  x
A =  3  5  2   =>  0  x  x
     1  0  4       0  0  x

|A| = 2 (20 - 0) -4 (12 - 2) + 7 (0 - 5)
    = 2 (20) -4 (10) +7 (-5)
    = 40-40-35
    = -35


Carmer's rule:-

1. |A|!=0 ,|Ax|=|Ay|=|Az|!=0  ---> unique solution

2. |A|=0 ,|Ax|=|Ay|=|Az|!=0   ---> NO solution

3. |A|=0 ,|Ax|=|Ay|=|Az|=0    ---> many/infinite solution


x+y+2z = 4, 2x+2y+4z = 8, 3x+3y+6z = 10

   1  1  2   x   4
=> 2  2  4 * y = 8
   3  3  6   z   10

|A|  = 0

|Ax| = 0

|Ay| = 0

|Az| = 0

infinite solutions

z = k

x-y   = 4 - 2k
2x+2y = 8 - 4k
3x+3y = 10 - 6k

 2x - 2y =  8 - 4k
 2x + 2y =  8 - 4k
-   -      -  + 

x=0
y=4 - 2k
z=k
(x,y,z) = (0,4-2k,k) k belongs to any real number

why matrix?

input data => text, number, image/video, audio

image/video => video (FPS) 

image => HD image 1080x1920 -> resolution
pixel -> dot per inch(dpi)

colors => 16M

primary colors = RGB
Secondary colors = CYM

white,black

(12,45,136)

text->conversion->vector

conversion method-> word embedding and word2vec

word embedding-> pre trained embedding -> GLoVe

word2vec -> count vectorizer -> based on frequency (n-gram/skip gram)

n=1 --> unigram, n=2 --> bigram

"My friend name is John and his pet's name is tommy."

processed text:-

"friend name John pet name tommy" ==> [(0,1),(1,2),(2,1),(3,1),(4,1)]

	unigram				bigram

0 (friend,(0,1))		friend name (0,1)
1 name (0,2)			name john (0,1)
2 john (0,1)			john pet  (0,1)
3 pet (0,1)			pet name  (0,1)
4 tommy (0,1)			name tommy (0,1)

Types of matrix

row matrix -> [1,2,3]

column -> | 2 |
	  | 4 |
	  | 5 |

square matrix -> m x n if m=n


A.At = 1

Decomposition

12 -> 3 x 4 ,6 x 2

{3,2}

1044

Square Matrix -> Eigen Decomposition

Non-Square Matrix -> SVD (singular value decomposition)

PCA - principle component analysis

Problems in AI:-

1. Poor conditioning ==> x -> y, x+1 -> y+1

2. Overfit and underfit

3.  

---------------------------------------------------------------------------------------------------------------------------
ML Algorithms:-

01. Linear regression -> regression (completed)
02. Logistic regression -> Classification (completed)
03. SVM -> Classification (completed)
04. K-means -> clustering (completed)
05. KNN  -> clustering (completed)
06. Hierarchy clustering -> clustering (completed)
07. LDA -> topic modeling (completed)
08. TF-IDF -> topic modeling (completed)
09. Eclat -> associative learning (completed)
10. Apriori -> associative learning (completed)
11. Random forest -> Ensemble method (completed)
12. AdaBoost -> Ensemble method (completed)
13. Decision tree -> Classification and regression (completed)
14. Naive Baye's -> Classification  and regression (completed)	
15. PCA -> Dimensionality reduction (coding is not yet completed)

Neural Network:-

1. perceptron
2. sigmoid
3. multi layer preceptron
4. Backpropagation
4.a gradient descent
4.b stochostic gradient descent
5. Vanila neural network

Deep Learning:-
1. Optimization of Neural network
2. Terminology in DL
3. CNN
4. RNN
5. LSTM
6. GRU
7. Bi-LSTM

Projects:-
1. 

Steps:-
1. Load dataset
2. Split dataset (70:30)
3. Define an algorithm
4. Training (70%)
5. Validation - accuracy = 100%
6. Tuning
7. Testing (30%) >= 85%

Assume:-
1. Application
2. Algorithms
3. Datasets

---------------------------------------------------------------------------------------------------------------------------
1. PCA

number of person -> 8
p=8

each person has 5 images ==> M = 40

single image size => 3 x 3 -> 9 x 1

5 images size => 45 x 1

overall matrix => 45 x 8

mean image size => 45 x 1


optimum image processing size => 28 x  28, 64 x 64, 96 x 96, 

45 x 8 . 8 x 45 = 45 x 45

8 x 45 . 45 x 8 = 8 x 8

eigen vector(8 x 8)

eigen space

norm or norm distance

http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/

---------------------------------------------------------------------------------------------------------------------------

2. Linear Regression

y = mx+c

y -> output
m -> slope
c -> intercept
x -> input

y^ = b0 + b1x => linear regression
y^ = b0 + b1x1+b2x2+b3x3+....   => multilinear regression/least regression

y  -> actual output
y^ -> predicted output

y-y^ = 0  --> error

error
-> absolute error (y-y^)
-> Mean absolute error (y-y^)/n
-> mean squared error ((y-y^)^2)/n
-> Root mean squared error (((y-y^)^2)/n)^1/2


gaussian regression,multilinear regression,least square regression,

example:- (3,7.5),(4,11.5),(6,18)(7,?)

y^ = b0+3b1
y^ = b0+4b1  -> -4.5+4(4) = -4.5+16 = 11.5

y-y^ = 0

7.5-b0-3b1=0
11.5-b0-4b1=0

solving those two equation 

  7.5-b0-3b1=0
  11.5-b0-4b1=0
-     +   +  
-------------------
-4+b1=0

b1=4

7.5-b0-3(4)=0
7.5-12=b0

b0 = -4.5

y^ = -4.5+6(4) = -4.5+24=19.5

18-19.5 = 1.5

y^ = -4.5+7(4) = -4.5+28 = 23.5


Dataset:- Advertising.csv
inputs:- TV,Radio,Newspaper
output:- sale 

usecase:-

1. To predict the future sale value for future inputs.
2. To analyze the ratio of inputs


---------------------------------------------------------------------------------------------------------------------------
2. Logistic Regression

classification category

reason for regression ->  from linear regression 

logit function => log(p/1-p)

inverse of logit => logistic or sigmoid 

logistic => 1/1+exp(-z)

z = - infinity and infinity

exp(-z) = is always positive 

1/exp(-z) = 0 to 1 

0 to 0.5 => class A
0.5 to 1 => class B

z = b0 + b1x

Dataset:-
	IMDb movie review 

Class:-
	1 -> positive
	0 -> negative
use case:-
	sentiment analysis


Example:-

I bought a book from amazon which costs 100$ which is so good to read.  -> positive 

Data preprocessing:-

HTML tags, special characters,numbers -> to be removed
to lower case

---------------------------------------------------------------------------------------------------------------------------
3. K-Means

unsupervised -> only input -> clustering category -> segementation


Vector distance -> Euclidean distance 

K-> no of clusters (default is 8 in sklearn)

k-Means -> hard clustering
	-> Top to bottom approach 


mean Euclidean distance = 0

---------------------------------------------------------------------------------------------------------------------------
4. KNN -> K-nearest neighbours (clustering/classification)

K-> no of clusters (default is 8 in sklearn)

we don't find center value/centroid
we don't calculate mean value

---------------------------------------------------------------------------------------------------------------------------
task:-

twitter dataset -> sentiment analysis -> classsification model 
create short words dictionary.

algorithms:-
	logistic, KNN -> train and test (cluster),SVM,radom forest,adaboost and its variance, Naive Baye's,decision tree

usa housing dataset -> price predict -> regression model
	ridge,lasso,elasticnet,huber,linear regression,radom forest,adaboost and its variance, Naive Baye's(BayesianRidge),decision tree

Interview questions:-

https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/

https://www.springboard.com/blog/machine-learning-interview-questions/

https://www.digitalvidya.com/blog/machine-learning-interview-questions/

Decision Tree task:

predict the sentiment of twitter tweets using decision tree by using both gini index and entropy and calculate the time taken and accuracy of the both model.
----------------------------------------------------------------------------------------------------------------------------------
5. Hierarchy clustering


bottom to top approach

a,b,c,d,e

(a),(b),(c),(d),(e)
((a,b)c) ((d)e)

apple,mango,pine apple, orange , banana

linkage method:- (complete, average and single/simple)
	all features,single feature or average features

-> All features -> taste, color, shape, size, seed

Dendrogram -> relationship 

----------------------------------------------------------------------------------------------------------------------------------
6. SVM

SVM -> Support vector machine (binomial classifier)[A / B]

A B

one to one -> A / B

A B C

one to many -> A / BC --> B / C

A B C D

many to many -> AB / CD 
		 |   |
		A/B C/D

border
	 |- soft margin
margin ->
	 |- hard margin

hyperborder
gamma
kernel
----------------------------------------------------------------------------------------------------------------------------------
7. Apriori & Eclat

library:-
	Apriori -> mlxtend
        Eclat   -> PyFim, fim

based on frequent pattern
apriori -> width search method
	-> high computational power and time
	-> Horizontal method

eclat -> depth search method
      -> less computational power and time
      -> vertical method


			total number of transactions an item presents
support (product A) =---------------------------------------------------------
				total number of transactions



support number = total number of transactions an item presents

min_sup = threshold

Apriori 

product        support number
-------------------------------------
onion		 4
potato		 5
burger		 4
milk		 4
beer		 3

min_sup = 50% = 3

5C2 = 10

500c2=124750

ncr => n! /(n-r)!r! = 5!/3!2!
  5*4*3*2*1	
=------------ = 10
  3*2*1 2*1


combo product        support number
-------------------------------------
   O,P			4
   O,B			3
   o,m			2
   o,br			2
   P,B			4
   P,M			3
   p,br			2
   b,m			2
   b,br			2
   m,br			2

Eclat

onion potato burger milk beer
  t1    t1     t1    t2   t3
  t4    t2     t2    t3   t5
  t5    t4     t5    t4   t6
  t6    t5     t6    t6
        t6

(onion).intersect(potato) => t1,t4,t5,t6

----------------------------------------------------------------------------------------------------------------------------------
8. Random forest

Ensemble method -> collective 

collection different set of algorithms.

-> average
-> Max voting 

Classification output:-

A = 0.84, B = 0.07 ,C = 0.09

ridge 91.56
elasticnet 92.41
linear  92.75


SVM A
Logistic B
KNN B

B - 2
A - 1

n_estimator (sklearn) => no of trees 

sklearn -> base estimator = decision tree
----------------------------------------------------------------------------------------------------------------------------------
9. Adaboost 

weaker -> weaker -> stronger

input   :- x1    x2   x3

co-eff(W)   :- 0.2  0.5  0.01
 
Ex output:- -1   1   -1

predicted:- -1  -1   -1

terror   :-  0   1    0

error   = sum(w(i)*terror)/sum(w(i))

	= ((0.2*0)+(0.5*1)+(0.01*0))/(0.2+0.01+0.5)
	
	= 0.5/0.71 
	
error,E = 0.704
	
stage   = ln((1-E)/E)

	= ln(0.296/0.704)
	
	= -0.866

W_new	= w_old + exp(terror*stage)
----------------------------------------------------------------------------------------------------------------------------------
10. Naive Baye's

conditional probability

			P(A) P(B/A)
Navie Baye's, P(A/B) = --------------
			   P(B)

Dataset:- 1_Bn3d4Z62sof3K4U1_0pSlQ.jpeg
 
Test case :- SMHT -> encoded version = 2111

output :- play? yes/no

		P(yes) P(smht/yes)
P(yes/smht) = ----------------------
		     P(smht)

		P(no) P(smht/no)
P(no/smht) = ----------------------
		     P(smht)

           9
P(yes) = ----
	  14
	  5
P(no) = ----
	 14

P(smht) =  P(s)*P(m)*P(h)*P(t)
           5     6     7     6
	= --- * --- * --- * --- 
	  14    14     14   14

	   	   2     4     3     3
P(smht/yes) 	= --- * --- * --- * --- 
		   9     9     9     9

	   	   3     2     4     3
P(smht/no) 	= --- * --- * --- * --- 
		   5     5     5     5

P(yes/smht) = 0.2135

P(no/smht) = 1.2537

	     P(yes/smht)                 0.2135
P(yes) = ----------------------   = ----------------- =  0.146             
	 P(yes/smht)+P(no/smht)      0.2135 + 1.2537


	     P(no/smht)                1.2537
P(no) = ---------------------- = ----------------- =  0.854            
	 P(yes/smht)+P(no/smht)    0.2135 + 1.2537

Output:- No

Variances:-
	Gaussian navie Baye's
	Mulitnominal NB
	Binominal NB
----------------------------------------------------------------------------------------------------------------------------------
11. Decision tree
	
1. CART -> classification and regresion tree -> gini index

2. ID3 -> iterative dichotomiser 3 -> entropy and information gain

Entropy 

H = -P log2 P

outlook 

        -2	    2	  -3          3
sunny = --- * log2 ---  + --- * log2 ---
	 5          5      5          5
      =	0.971

	   -4          4
overcast = --- * log2 ---
	    4	       4
	 = 0

        -2	    2	  -3          3
rainy = --- * log2 ---  + --- * log2 ---
	 5          5      5          5	
      = 0.971

	    5	          4         5
outlook =  --- * -0.27 + --- * 0 + --- * -0.27
	    14            14        14
	= 0.693

H(outlook) = 0.693


Humidity


H(humidity) = 0.7916


Play


H(play) = 0.9402
		

information gain(outlook) = H(play) - H(outlook) = 0.9402 - 0.693 = 0.247
information gain(humidity) = H(play) - H(humidity) = 0.9402 - 0.7916 = 0.148

gini index = 1 - sum(P_t)^2
----------------------------------------------------------------------------------------------------------------------------------
12. TF-IDF

term frequency- inverse document frequency

term -> word
document -> line of sentence/sentence
corpus -> paragraph
similarity -> vector distance

Topic modeling

1. TF-IDF
2. LDA
3. LSI


tfidf weight = tf * idf

	   N
idf = log(----)
	   df

tf = (1+log(1+tf))

				    N
tfidf weight = (1+log(1+tf)) * log(----)
				    df

-----------------------------------------------------------------------------------------------------------------------------------
13. Latent Dirichlet allocation

parent matrix  => document(d) vs term(w)

	w1	w2
d1      1	2

d2	0	1

d3	2	1


child matrices => topic(t) vs term(w) , document(d) vs topic(t)

	w1	w2
t1	0.6	0.9

t2	0.2	0.6

	t1	t2
d1      

d2

d3
-----------------------------------------------------------------------------------------------------------------------------------
Neural Network

1. perceptron 
 single neuron network

to take a decision -> distance,weather,items

weights -> priority given to the decision parameter

bias -> threshold

weights and bias both are initialised randomly.

distance = 0.4
weather = 0.25
items = 0.35

threshold = 0.65

x1*w1+x2*w2+x3*w3 < threshold => no
                 >= threshold => yes

scenario 1:-

x1 = 1 (distance is less)
x2 = 0 (weather is bad)
x3 = 1 (items are more)

actual :- yes

1*0.4+0*0.25+1*0.35 = 0.4+0+0.35 = 0.75 >= 0.65 => yes


scenario 2:-

x1 = 0 (distance is high)
x2 = 1 (weather is good)
x3 = 1 (items are more)

actual :- yes

0*0.4+1*0.25+1*0.35 = 0.25+0.35 = 0.6 < 0.65 => no


updating weights:

distance = 0.4
weather = 0.275
items = 0.375

threshold = 0.65

scenario 2 (updated weights)


x1 = 0 (distance is high)
x2 = 1 (weather is good)
x3 = 1 (items are more)

actual :- yes

0*0.4+1*0.275+1*0.375 = 0.275+0.375 = 0.65 >= 0.65 => yes

for any linear systems, 

	 x   ->  y
	x+x' -> y+y'	

2. Sigmoid neurons -> perceptron with activation function

since perceptron is nonlinear in nature which inverts the output completely for a small change in the weights.

recent trends in activation functions are ReLu,softmax,


3. Multilayer perceptron

D = 0.8,0.2,0.5

W = 0.2,0.8,0.5

4. Vanila neural network:
Activation function :- sigmoid
Optimizer :- SGD 

Neural network case study:-

hand written digits recoginition 

	Dataset:- MNIST digits dataset -> Training data :- 60,000, Testing Data :- 10,000, number of class :- 10
		input image size :- 28 x 28
					
	Algorithm :- Vanila Neural Network
	Activation function :- sigmoid
	Optimizer :- SGD
	
output class:- 5 => 0000010000
	       7 => 0000000100



